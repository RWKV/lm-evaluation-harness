name: GH Task Runner (Fast Suite)
on:
  workflow_dispatch:
    inputs:
      approval_notice:
        description: 'WARNING: This will spin up a large number of tasks - get approval from admin before running'
        required: false
        default: 'NOT_APPROVED'
        type: choice
        options:
        - NOT_APPROVED
        - APPROVED
      model_hf_repo:
        description: 'Model Hugging Face Repository'
        required: true
        default: 'RWKV/rwkv-5-world-1b5'
      model_args:
        description: 'Model Arguments (ie: dtype="fp16")'
        required: false
        default: 'trust_remote_code=True'
      batch_size:
        description: 'Batch Size'
        required: true
        default: 'auto'
      backend:
        description: 'Backend to use'
        required: true
        default: 'nvidia-gpu'
        type: choice
        options:
        - nvidia-gpu
        - intel-gpu
        - amd-gpu
        - any-gpu
      gpu_vram:
        description: 'Minimum GPU VRAM (ignored for MPS)'
        required: true
        default: '24'
        type: choice
        options:
        - 16
        - 24
        - 40
        - 48
        - 80
      num_fewshot:
        description: 'num_fewshot setting (ignored if < 0)'
        required: true
        default: -1

env:
  # Get the final task
  RUN_TASK: ${{ github.event.inputs.custom_task || github.event.inputs.run_task }}

  # HF repo to sync to
  HF_REPO_SYNC: rwkv-x-dev/lm-eval-output

  # Model HF repo
  MODEL_HF_REPO: ${{ github.event.inputs.model_hf_repo }}

  # Secrets
  HUGGING_FACE_HUB_TOKEN: ${{secrets.HUGGING_FACE_HUB_TOKEN}}

jobs:
  gh-task-runner-large-suite-1:

    # Check for approval notice
    if: ${{ github.event.inputs.approval_notice == 'APPROVED' }}

    # Strategy Matrix
    strategy:
      # Disable fail-fast behavior
      fail-fast: false 
      matrix:

        # NOTE: There is a matrix limit of 256 on github
        run_task:
          ### Known Problematic tasks (various error reasons - file missing, temp=0, etc)
          ### ----------------------------------------------------------------------------
          ## ----
          ## Need to double check (removed before i started organizing by failure reasons)
          ## ----
          # - arithmetic_*
          # - asdiv
          # - bbh
          # - bbh_cot_fewshot
          # # - bbh_cot_fewshot_*
          # - bbh_cot_zeroshot
          # # - bbh_cot_zeroshot_*
          # - bbh_fewshot
          # # - bbh_fewshot_*
          # - bbh_zeroshot
          # # - bbh_zeroshot_*
          # - bigbench_*
          # - boolq-seq2seq
          ## ----
          ## temp=0 issues
          ## ----
          # - anagrams*
          # - babi
          # - code2text_*
          # - codexglue_code2text
          # - coqa
          # - cycle_letters
          # - drop
          # - random_insertion
          # - unscramble
          # - super-glue-*
          # - squadv2
          # - scrolls
          # - reversed_words
          # - qasper
          # # - qasper_*
          ## ----
          ## Does not exists / file 404s / broken links
          ## ----
          # - csatqa
          # - csatqa_*
          # - belebele
          # # - belebele_*
          # - generate_until
          # - polemo2
          # # - polemo2_*
          # - pile
          # # - pile_*
          ## ----
          ## Wierd package dependencies
          ## ----
          # #### ifeval requires `pip install langdetect immutabledict nltk` (not documented)
          # # However even after all that, it gives an "missing index error"
          # - ifeval
          # #### minerva_math: antlr4 (not on pip?)
          # - minerva_math
          # # - minerva_math_*
          ## ----
          ## Requires hugging face login
          ## ----
          # - toxigen
          ## ----
          ## Wierd errors (need to reinvestigate)
          ## ----
          # - gpt3_translation_benchmarks
          # - gsm8k_cot
          # - headqa
          # # - headqa_en
          # # - headqa_es
          # - hellaswag_*
          # - iwslt2017
          # # - iwslt2017-*
          # - wmt-ro-en-t5-prompt
          # - wmt-t5-prompt
          # - t0_eval
          # - storycloze
          # # - storycloze_*
          # - self_consistency

          ### Commented out tasks, are rolled up in their larger non _* varients
          ### ============================================================================

          ### Extra long running tests (>=4hrs on 4x3090)
          ### Reserved for 8xGPU nodes, put up top for priority
          ### ----------------------------------------------------------------------------
          - chain_of_thought
          - flan_held_*
          - fld
          # - fld_*
          - translation
          - gsm8k_cot_self_consistency

          # This takes over 24 hours on 4x3090s even ??
          - realtoxicityprompts

          ### Slow tasks (>=1 hr on 4x3090)
          ### ----------------------------------------------------------------------------
          - wmt14
          # - wmt14-*
          - wmt16
          # - wmt16-*
          - math_word_problems


    # Name of the job
    name: "[${{ matrix.run_task }}] ${{ github.event.inputs.model_hf_repo }} - ${{ github.event.inputs.model_args }}"

    # Due to github worker hard limitation, of 24 hours
    # we apply a timeout of 23 hours instead.
    timeout-minutes: 1380

    # Select the type of runner that the job will run on
    runs-on: 
      - ${{ github.event.inputs.backend }}
      - gpu-vram-${{ github.event.inputs.gpu_vram }}
      - ${{ matrix.gpu_count }}

    # Actual task setup, and run steps
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: nvidia-smi check (for easy debugging)
        run: nvidia-smi

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11' 
          
      - name: Install dependencies / setup project
        run: |
          # Basic dependencies install, and output setup
          mkdir -p ./output
          python -m pip install .
          python -m pip install -e .

          # Needed for ifeval
          python -m pip install langdetect immutabledict

          # Setup HF cache
          chmod +x ./gh-task-runner/*.sh
          ./gh-task-runner/hf-cache-setup.sh 

      - name: Run Task
        run: |
          # Run it
          echo "# ------------------------------"
          echo "# Running Task ...."
          echo "# ------------------------------"

          # Get the final task to run
          task_to_run=${{ matrix.run_task }}

          # Check if the few shot setting is larger or euqal to 0
          if [ ${{ github.event.inputs.num_fewshot }} -ge 0 ]; then
            # Fail on pipe error
            set -o pipefail

            # Run it
            accelerate launch -m lm_eval --model hf \
            --model_args pretrained=${{ github.event.inputs.model_hf_repo }},${{ github.event.inputs.model_args }} \
            --tasks $task_to_run \
            --batch_size ${{ github.event.inputs.batch_size }} \
            --num_fewshot ${{ github.event.inputs.num_fewshot }} \
            --log_samples --output_path ./output 2>&1 | tee -a ./output/taskrun.log
          else
            # Fail on pipe error
            set -o pipefail

            # Run it
            accelerate launch -m lm_eval --model hf \
            --model_args pretrained=${{ github.event.inputs.model_hf_repo }},${{ github.event.inputs.model_args }} \
            --tasks $task_to_run \
            --batch_size ${{ github.event.inputs.batch_size }} \
            --log_samples --output_path ./output 2>&1 | tee -a ./output/taskrun.log
          fi

      ########################################################################
      # We disable HF upload for large runs, as it WILL hit the rate limits
      ########################################################################
      # - name: Upload outputs to HF
      #   if: always()
      #   run: |
      #     CLEANED_TASK=$(echo "${{ matrix.run_task }}" | sed 's/\*/_/g')
      #     HF_SUBDIR_PATH="${{ env.MODEL_HF_REPO }}/$CLEANED_TASK/${{ github.event.inputs.model_args }}-num_fewshot=${{ github.event.inputs.num_fewshot }}/${{ github.event.inputs.backend }}/"
      #     ./gh-task-runner/hf-upload-runner.sh "${{ env.HF_REPO_SYNC }}" "$HF_SUBDIR_PATH" "./output"

      ########################################################################
      # Instead we adjust the format for GH-Upload
      ########################################################################
      - name: Change to GH-Upload format
        if: always()
        run: |
          CLEANED_TASK=$(echo "${{ matrix.run_task }}" | sed 's/\*/_/g')
          HF_SUBDIR_PATH="${{ env.MODEL_HF_REPO }}/$CLEANED_TASK/${{ github.event.inputs.model_args }}-num_fewshot=${{ github.event.inputs.num_fewshot }}/${{ github.event.inputs.backend }}/"
          
          # Move the files
          mkdir -p "./gh-upload/$HF_SUBDIR_PATH"
          mv ./output/* "./gh-upload/$HF_SUBDIR_PATH"

      - name: Save output Files
        uses: actions/upload-artifact@v3
        # if: failure()
        if: always()
        with:
          name: output-files
          path: |
            gh-upload/*
          retention-days: 90

      